#mpiexec -n 4 python testMPIxN.py
#mpiexec -n 4 python testMPIxN.py N M
# get non-MPI reference time using testMonolithic.py


'''
This script simulates spheres falling on a plate using a distributed memory approach based on mpi4py
The number of spheres assigned to one particular process (aka 'worker') is N*M, they form a regular array.
The master process (rank=0) has no spheres assigned presently; it is in charge of getting the total force on the plate (in more advanced simulations it could handle boundary conditions).
The number of subdomains depends on argument 'n' of mpiexec. Since rank=0 is not assigned a regular subdomain the total number of spheres is n*N*M


The logic is as follows:
1. Instanciate a complete, ordinary, yade scene with the plate and spheres (only done by rank=0)
2. Insert subdomains as special yade bodies. This is somehow similar to adding a clump body on the top of clump members
3. Broadcast this scene to all workers. In the initialization phase the workers will:
	- define the bounding box of their assigned bodies and return it to other workers
	- detect which assigned bodies are virtually in interaction with other domains (based on their bounding boxes) and communicate the lists to the relevant workers
	- erase the bodies which are neither assigned nor virtually interacting with the subdomain
4. Run a number of 'regular' iterations without re-running collision detection (verlet dist mechanism)
5. In each regular iteration the workers will:
	- calculate internal and cross-domains interactions
	- execute Newton on assigned bodies (modified Newton skips other domains)
	- send updated positions to other workers and partial force on floor to master

Yet to be implemented is the global update of domain bounds and new collision detection. It could be simply achieved by importing all bodies back in master process and re-running an initialization/distribution but there are certainly mmore efficient techniques to find.

Performance:
The default settings of testMPIxN.py and testMonolithic.py correspond to 90k spheres and 3 subdomains, for this problem size the MPI version runs approximately as fast as monolithic (actually a bit faster), the performance is expected to increase for more threads and/or more bodies per thread

Hints:
- handle subD.intersections with care (same for mirrorIntersections). subD.intersections.append() will not reach the c++ object. subD.intersections can only be assigned (a list of list of int)


'''

import sys
import time
from mpi4py import MPI
import pickle #just for measuring size of message before comm.irecv
import numpy as np

this = sys.modules[__name__]

comm = MPI.COMM_WORLD
rank = comm.Get_rank()
numThreads = comm.Get_size()

NSTEPS=100 #turn it >0 to see time iterations, else only initilization
ACCUMULATE_FORCES=True #control force summation on master's body. FIXME: if false master goes out of sync since nothing is blocking rank=0 thread
VERBOSE_OUTPUT=False
SEND_SHAPES=False #if false only bodies' states are communicated between threads, else shapes as well (to be implemented)
ERASE_REMOTE = True #erase bodies not interacting wit a given subdomain? else keep dead clones of all bodies in each scene
OPTIMIZE_COM=True
USE_CPP_MPI=True and OPTIMIZE_COM
YADE_TIMING=True #report timing.stats()?

#tags for mpi messages
_SCENE_=11
_SUBDOMAINSIZE_=12
_INTERSECTION_=13
_ID_STATE_SHAPE_=14
_FORCES_=15
_MIRROR_INTERSECTIONS_ = 16

#for coloring processes outputs differently
class bcolors:#https://stackoverflow.com/questions/287871/print-in-terminal-with-colors
    HEADER = '\033[95m'
    OKBLUE = '\033[94m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'

def mprint(m): #this one will print regardless of VERBOSE_OUTPUT
	if 1:
		if rank>0:
			print (bcolors.WARNING if rank==1 else bcolors.OKBLUE) +"worker"+str(rank)+": "+m+bcolors.ENDC
		else: print bcolors.HEADER+"master:" +m+bcolors.ENDC

def wprint(m):
	if not VERBOSE_OUTPUT: return
	mprint(m)


#from os.path import expanduser
#sys.path.append(expanduser('~')+'/yade/bin') # path where you have yadeimport.py
#print expanduser('~')+'/yade/bin'
# yadeimport.py is generated by `ln yade-versionNo yadeimport.py`, where yade-versionNo is the yade executable
from yadeimport import yade,sphere,box,Sphere,Body,Subdomain,Bo1_Subdomain_Aabb,typedEngine,PFacet,GridConnection,GridNode,PyRunner,kineticEnergy
#from yadeimport import yade,Body,Subdomain,Bo1_Subdomain_Aabb,typedEngine,PyRunner #mandatory
#from yadeimport import PFacet,GridConnection,GridNode #optional (for special handling of those objects)
#from yadeimport import userSession



#from yadeimport import *


#RULES:
	#- intersections[0] has 0-bodies (to which we need to send force)
	#- intersections[thisDomain] has ids of the other domains overlapping the current ones
	#- intersections[otherDomain] has ids of bodies in _current_ domain which are overlapping with other domain (for which we need to send updated pos/vel)


def receiveForces(subdomains):
	'''
	Accumulate forces from subdomains (only executed by master process), should happen after ForceResetter but before Newton and before any other force-dependent engine (e.g. StressController), could be inserted via yade's pyRunner
	'''
	if 0: #non-blocking:
		reqForces=[]
		for sd in subdomains:#would be better as a loop on subdomains directly, but we don't have those
			
			#wprint( "master getting forces from "+str(b.subdomain)+"(id="+str(b.id)+")")		
			reqForces.append(comm.irecv(None,sd, tag=_FORCES_))
			#wprint( "master got forces from "+str(b.subdomain)+": "+str(forces))		
		for r in reqForces:
			forces=r.wait()
			for ft in forces:		
				#wprint(  "adding force "+str(ft[1])+" to body "+str(ft[0]))
				O.forces.addF(ft[0],ft[1])
				O.forces.addT(ft[0],ft[2])
	else:
		for sd in subdomains:#would be better as a loop on subdomains directly, but we don't have those
			forces=comm.recv(source=sd, tag=_FORCES_)
			#wprint( "master got forces from "+str(b.subdomain)+": "+str(forces))		
			for ft in forces:
				#wprint(  "adding force "+str(ft[1])+" to body "+str(ft[0]))
				O.forces.addF(ft[0],ft[1])
				O.forces.addT(ft[0],ft[2])

def unboundRemoteBodies():
	'''
	Turn bounding boxes on/off depending on rank
	'''
	for b in O.bodies:# unbound the bodies assigned to workers (not interacting directly with other bodies in master scene)
		if not b.isSubdomain and b.subdomain!=rank:
			b.bounded=False
			b.bound=None
			
def reboundRemoteBodies(ids):
	'''
	update states of bodies handled by other workers, argument 'states' is a list of [id,state] (or [id,state,shape] conditionnaly)
	'''
	for id in ids:
		b = O.bodies[id]
		if not isinstance(b.shape,GridNode): b.bounded=True 

def updateDomainBounds(subdomains): #subdomains is the list of subdomains by body ids
	'''
	Update bounds of current subdomain, broadcast, and receive updated bounds from other subdomains
	Precondition: collider.boundDispatcher.__call__() 
	'''
	wprint( "updating bounds: "+str(subdomains))
	sharedBounds=[None] #initialize with None for master process, to aligned with rank
	buf = []	
	
	for sdId in subdomains:
		if O.bodies[sdId].subdomain!=rank:
			wprint("receiving from "+str(O.bodies[sdId].subdomain))
			buf.append(bytearray(1<<8))
			#comm.isend([subD.boundsMin,subD.boundsMax], dest=worker, tag=_SUBDOMAINSIZE_)
			req=comm.irecv(buf[-1],O.bodies[sdId].subdomain, tag=_SUBDOMAINSIZE_)
			sharedBounds.append(req)
		else: sharedBounds.append(None) #to keep buf aligned with rank
	if rank!=0:#this is not master process, update bounds and share
		subD=O.bodies[subdomains[rank-1]].shape #shorthand to shape of current subdomain
		subD.setMinMax()
		for worker in range(numThreads):
			if worker!=rank:
				#wprint("sending "+str([subD.boundsMin,subD.boundsMax]))
				#comm.isend([subD.boundsMin,subD.boundsMax], dest=worker, tag=_SUBDOMAINSIZE_)
				comm.send([subD.boundsMin,subD.boundsMax], dest=worker, tag=_SUBDOMAINSIZE_)
				#sharedBounds.append(req) #keep track of non-blocking messages sent 
	for sdId in subdomains:
		if O.bodies[sdId].subdomain!=rank:
			minmax=sharedBounds[O.bodies[sdId].subdomain].wait()
			#minmax=comm.recv(source=O.bodies[sdId].subdomain, tag=_SUBDOMAINSIZE_)
			wprint( "received mn,mx="+str(minmax[0])+" "+str(minmax[1])+" from "+str(O.bodies[sdId].subdomain))
			O.bodies[sdId].shape.boundsMin, O.bodies[sdId].shape.boundsMax = minmax[0],minmax[1]
	wprint( "receiving bounds")	
	#for req in sharedBounds: req.Wait()
	wprint( "bounds updated")
	

def maskedPFacet(pf, boolArray):
	'''
	List bodies within a facet selectively, the ones marked 'True' in boolArray (i.e. already selected from another facet) are discarded
	'''
	l=[]
	for id in [pf.node1.id, pf.node2.id, pf.node3.id, pf.conn1.id, pf.conn2.id, pf.conn3.id]:
		if not boolArray[id]:
			l.append(id)
			boolArray[id]=True

def maskedPFacet(b, boolArray):
	'''
	List bodies within a facet selectively, the ones marked 'True' in boolArray (i.e. already selected from another facet) are discarded
	'''
	l=[]
	pf=b.shape
	for id in [b.id,pf.node1.id, pf.node2.id, pf.node3.id, pf.conn1.id, pf.conn2.id, pf.conn3.id]:
		if not boolArray[id]:
			l.append(id)
			boolArray[id]=True
	return l

def maskedConnection(b, boolArray):
	'''
	List bodies within a facet selectively, the ones marked 'True' in boolArray (i.e. already selected from another facet) are discarded
	'''
	l=[]
	pf=b.shape
	for id in [b.id,pf.node1.id, pf.node2.id]:
		if not boolArray[id]:
			l.append(id)
			boolArray[id]=True
	return l

def genLocalIntersections(subdomains):
	'''
	Defines sets of bodies within current domain overlapping with other domains.
	The structure of the data for domain 'k' is:
	[[id1, id2, ...],  <----------- ids of bodies in domain k interacting with master domain (subdomain k itself excluded)
	 [id3, id4, ...],  <----------- ids of bodies in domain k interacting with domain rank=1 (subdomain k itself excluded)
	 ...
	 [domain1, domain2, domain3, ...], <---------- rank (not id!) of external domains interacting with domain k
	 ...
	 ]
	'''
	intersections=[[] for n in range(numThreads)]
	for sdId in subdomains:
		#grid nodes or grid connections could be appended twice or more, as they can participate in multiple pfacets and connexions
		#this bool list is used to append only once
		appended = np.repeat([False],len(O.bodies))
		subdIdx=O.bodies[sdId].subdomain
		intrs=O.interactions.withBodyAll(sdId)
		#special case when we get interactions with current domain, only used to define interactions with master, otherwise some intersections would appear twice
		if subdIdx==rank:
			for i in intrs:
				otherId=i.id1 if i.id2==sdId else i.id2
				b=O.bodies[otherId]
				if b.subdomain==0:
					if isinstance(b.shape,PFacet):
						intersections[0]+= maskedPFacet(b, appended); continue
					if isinstance(b.shape,GridConnection):
						intersections[0]+=maskedConnection(b, appended); continue
					#else (standalone body, normal case)
					intersections[0].append(otherId)
			if len(intersections[0])>0: intersections[subdIdx].append(0)
			continue
		# normal case
		for i in intrs:
			otherId=i.id1 if i.id2==sdId else i.id2
			b=O.bodies[otherId]
			if b.subdomain!=rank: continue
			if b.isSubdomain: intersections[rank].append(subdIdx) #intersecting subdomain (will need to receive updated positions from there)
			else:
				if isinstance(b.shape,PFacet):
						intersections[subdIdx]+= maskedPFacet(b, appended); continue
				if isinstance(b.shape,GridConnection):
						intersections[subdIdx]+=maskedConnection(b, appended); continue
				#else (standalone body, normal case)
				intersections[subdIdx].append(otherId)

		#for master domain set list of interacting subdomains (could be handled above but for the sake of clarity complex if-else-if are avoided for now)
		if rank==0 and len(intersections[subdIdx])>0:
			intersections[0].append(subdIdx)
	#wprint( "found "+str(len(intrs))+" intersections"+str(intersections))
	return intersections

def updateRemoteStates(states, setBounded=False):
	'''
	update states of bodies handled by other workers, argument 'states' is a list of [id,state] (or [id,state,shape] conditionnaly)
	'''
	ids=[]
	for bst in states:
		#print bst[0],O.bodies[bst[0]]
		ids.append(bst[0])
		b=O.bodies[bst[0]]
		b.state=bst[1]
		#if SEND_SHAPES: b.shape=bst[2]
		if setBounded and not isinstance(b.shape,GridNode): b.bounded=True 
	return ids

def genUpdatedStates(b_ids):
	'''
	return list of [id,state] (or [id,state,shape] conditionnaly) to be sent to other workers
	'''
	return [[id,O.bodies[id].state] for id in b_ids] if not SEND_SHAPES else [[id,O.bodies[id].state,O.bodies[id].shape] for id in b_ids]



#############   COMMUNICATIONS   ################"

def sendRecvStates():
	#____1. get ready to receive positions from other subdomains
	
	pstates = []
	buf = [] #heuristic guess, assuming number of intersecting is ~linear in the number of rows, needs
		
	if rank!=0: #the master process never receive updated states (except when gathering)
		for otherDomain in O.subD.intersections[rank]:
			#wprint( str(": getting states from ")+str(otherDomain))
			if not USE_CPP_MPI:
				buf.append(bytearray(1<<22)) #FIXME: smarter size? this is for a few thousands states max (empirical); bytearray(1<<24) = 128 MB 
				pstates.append( comm.irecv(buf[-1],otherDomain, tag=_ID_STATE_SHAPE_))  #warning leaving buffer size undefined crash for large subdomains (MPI_ERR_TRUNCATE: message truncated)
			else:
				O.subD.mpiIrecvStates(otherDomain) #use yade's messages (coded in cpp)
		#mprint("prepared receive: "+str(time.time()-start)); start=time.time()
	
	#____2. broadcast new positions (should be non-blocking if n>2, else lock) - this includes subdomain bodies intersecting the current one	
	reqs=[]
	for k in O.subD.intersections[rank]:
		if k==rank or k==0: continue #don't broadcast to itself... OTOH this list intersections[rank] will be used to receive
		#if len(b_ids)>0:#skip empty intersections, it means even the bounding boxes of the corresponding subdomains do not overlap
		#mprint("sending "+str(len(O.subD.intersections[k]))+" states to "+str(k))
		if not OPTIMIZE_COM:
			comm.send(genUpdatedStates(O.subD.intersections[k]), dest=k, tag=_ID_STATE_SHAPE_) #should be non-blocking if n>2, else lock?
		else:
			if not USE_CPP_MPI:
				reqs.append(comm.isend(subD.getStateValues(k), dest=k, tag=_ID_STATE_SHAPE_)) #should be non-blocking if n>2, else lock?
			else:
				O.subD.mpiSendStates(k)
		for r in reqs: r.wait() #empty if USE_CPP_MPI
		
	#____3. receive positions and update bodies
	
	if rank==0: return #positions sent from master, done. Will receive forces instead of states
	if not USE_CPP_MPI:
		nn=0	
		for ss in pstates:
			states=ss.wait()
			if not OPTIMIZE_COM:
				updateRemoteStates(states)
			else:
				subD.setStateValuesFromIds(O.subD.mirrorIntersections[O.subD.intersections[rank][nn]],states)
				nn+=1
	else:
		for otherDomain in O.subD.intersections[rank]:
			#mprint("listen cpp from "+str(otherDomain)+" in "+str(O.subD.intersections[rank]))
			#subD.mpiRecvStates(otherDomain)
			O.subD.mpiWaitReceived(otherDomain)
			O.subD.setStateValuesFromBuffer(otherDomain)


def isendRecvForces():
	'''
	Communicate forces from subdomain to master
	Warning: the sending sides (everyone but master) must wait() the returned list of requests
	'''	
	O.freqs=[]
	if ACCUMULATE_FORCES:
		if rank!=0:
			forces0=[[id,O.forces.f(id),O.forces.t(id)] for id in  O.subD.intersections[0]]
			#wprint ("worker "+str(rank)+": sending "+str(len(forces0))+" "+str("forces to 0 "))
			#O.freqs.append(comm.isend(forces0, dest=0, tag=_FORCES_))
			comm.send(forces0, dest=0, tag=_FORCES_)
			#mprint("broadcast forces: "+str(time.time()-start)); start=time.time()
		else: #master
			receiveForces(O.subD.intersections[0])

def waitForces():
	'''
	wait until all forces are sent to master. 
	O.freqs is empty for master, and for all threads if not ACCUMULATE_FORCES
	'''
	for r in O.freqs: r.wait()


##### INITIALIZE MPI #########

# Flag used after import of this module, turned True after scene is distributed
O.splitted=False


def splitScene():
	'''
	Split a monolithic scene into distributed scenes on threads
	precondition: the bodies have subdomain no. set in user script
	'''
	if rank == 0:
		

		O._sceneObj.subdomain=0
		O.subD=Subdomain() #for storage only, this one will not be used beyond that 
		subD= O.subD #alias
		#insert "meta"-bodies
		subD.subdomains=[] #list subdomains by body ids
		
		for k in range(1,numThreads):
			domainBody=Body(shape=Subdomain(ids=[b.id for b in O.bodies if b.subdomain==k]),subdomain=k) #note: not clear yet how shape.subDomainIndex and body.subdomain should interact, currently equal values
			domainBody.isSubdomain=True
			subD.subdomains.append(O.bodies.append(domainBody))

		#tell the collider how to handle this new thing
		collider.boundDispatcher.functors=collider.boundDispatcher.functors+[Bo1_Subdomain_Aabb()]
		collider.verletDist = 0.01 #FIXME: in general yade set this one automatically but here it will only happen after collider.__call__() (i.e too late...)
		
		#BEGIN Garbage (should go to some init(), usually done in collider.__call__() but in the mpi case we want to collider.boundDispatcher.__call__() before collider.__call__()
		collider.boundDispatcher.sweepDist=collider.verletDist;
		collider.boundDispatcher.minSweepDistFactor=collider.minSweepDistFactor;
		collider.boundDispatcher.targetInterv=collider.targetInterv;
		collider.boundDispatcher.updatingDispFactor=collider.updatingDispFactor;
		#END Garbage
		
		#distribute work
		for worker in range(1,numThreads):
			sceneAsString=yade.Omega().sceneToString()
			comm.send(sceneAsString, dest=worker, tag=_SCENE_) #sent with scene.subdomain=1, better make subdomain index a passed value so we could pass the sae string to every worker (less serialization+deserialization)
			
	else:
		O.stringToScene(comm.recv(source=0, tag=_SCENE_)) #receive a scene pre-processed by master (i.e. with appropriate body.subdomain's)  
		#print "worker 1 received",len(O.bodies),"bodies (verletDist=",collider.verletDist,")"
		O._sceneObj.subdomain = rank
		
		domainBody=None
		subdomains=[] #list of subdomains by body id
		for b in O.bodies:
			if b.isSubdomain:
				subdomains.append(b.id)
				if b.subdomain==rank: domainBody=b
			
		if domainBody==None: print "SUBDOMAIN NOT FOUND FOR RANK=",rank
		O.subD = domainBody.shape
		O.subD.subdomains = subdomains
		#subD= O._sceneObj.subD #alias
	subD = O.subD #alias
	
	#update bounds wrt. updated subdomain(s) min/max and unbounded bodies
	unboundRemoteBodies()
	collider.boundDispatcher.__call__()
	updateDomainBounds(subD.subdomains) #triggers communications
	collider.__call__() #see [1]
	subD.intersections=genLocalIntersections(subD.subdomains)
	
	
	#update mirror intersections so we know message sizes in advance
	subD.mirrorIntersections=[[] for n in range(numThreads)]
	if rank==0:#master domain
		
		for worker in range(1,numThreads):#FIXME: we actually don't need so much data since at this stage the states are unchanged and the list is used to re-bound intersecting bodies, this is only done in the initialization phase, though
			#states= [[id,O.bodies[id].state,O.bodies[id].shape] for id in intersections[worker]]
			#wprint("sending states to "+str(worker)+" ("+str(len(subD.intersections[worker]))+" bodies)")
			comm.send(subD.intersections[worker], dest=worker, tag=_MIRROR_INTERSECTIONS_)
			#comm.send(states, dest=worker, tag=_ID_STATE_SHAPE_) #sent with scene.subdomain=1
	else:
		# from master
		b_ids=comm.recv(source=0, tag=_MIRROR_INTERSECTIONS_)
		if len(b_ids)>0:
			reboundRemoteBodies(b_ids)
			subD.mirrorIntersections= [b_ids]+subD.mirrorIntersections[1:]
			# since interaction with 0-bodies couldn't be detected before, mirror intersections from master will
			# tell if we need to wait messages from master (and this is declared via intersections) 
			if not 0 in subD.intersections[rank]:
				temp=subD.intersections[rank]
				temp+=[0]
				subD.intersections=subD.intersections[:rank]+[temp]+subD.intersections[rank+1:]
			else:
				mprint("0 already in intersections (should not happen)")
		reqs=[]
		
		#from workers
		for worker in subD.intersections[rank]:
			if worker==0: continue #already received above
			#wprint("subD.intersections["+str(rank)+"]: "+str(subD.intersections[rank]))
			buf = bytearray(1<<22) #CRITICAL
			reqs.append([worker,comm.irecv(buf, worker, tag=_MIRROR_INTERSECTIONS_)])

		for worker in subD.intersections[rank]:
			if worker==0: continue #we do not send positions to master, only forces
			#wprint("sending "+str(len(subD.intersections[worker]))+" states to "+str(worker))
			comm.send(subD.intersections[worker], dest=worker, tag=_MIRROR_INTERSECTIONS_)
			#wprint("sent")

		nn=0
		for req in reqs:
			if subD.intersections[rank][nn]==0: nn+=1
			sd=subD.intersections[rank][nn]
			nn+=1
			#states=pstates.wait()
			#wprint("received "+str(len(states))+ " states")
			#ids=updateRemoteStates(states,True)
			intrs=req[1].wait()
			subD.mirrorIntersections= subD.mirrorIntersections[0:req[0]]+[intrs]+subD.mirrorIntersections[req[0]+1:]
			reboundRemoteBodies(intrs)
			
	if ERASE_REMOTE:
		numBodies = len(O.bodies)
		for id in range(numBodies):
			if not O.bodies[id].bounded and O.bodies[id].subdomain!=rank:
				connected = False #a gridNode could be needed as part of interacting facet/connection even if not overlaping a specific subdomain. Assume connections are always bounded for now, we thus only check nodes.
				if isinstance(O.bodies[id].shape,GridNode):
					for f in O.bodies[id].shape.getPFacets():
						if f.bounded: connected = True
					for c in O.bodies[id].shape.getConnections():
						if c.bounded: connected = True
				if not connected: O.bodies.erase(id)
				
	idx = O.engines.index(typedEngine("NewtonIntegrator"))
	moduleName = "mp"
	
	# append states communicator after Newton
	O.engines=O.engines[:idx+1]+[PyRunner(iterPeriod=1,command="sys.modules['yade.mpy'].sendRecvStates()",label="sendRecvStatesRunner")]+O.engines[idx+1:]
	
	# append force communicator before Newton
	O.engines=O.engines[:idx]+[PyRunner(iterPeriod=1,command="sys.modules['yade.mpy'].isendRecvForces()",label="isendRecvForcesRunner")]+O.engines[idx:]
	
	# append engine waiting until forces are effectively sent to master
	O.engines=O.engines+[PyRunner(iterPeriod=1,command="sys.modules['yade.mpy'].waitForces()",label="waitForcesRunner")]
	
	# mark scene splitted
	O.splitted=True
	
	


##### RUN MPI #########
def mpirun(nSteps):
	if not O.splitted: splitScene()
	if YADE_TIMING:
		O.timingEnabled=True
	O.run(nSteps,True)
	if YADE_TIMING:
		from yade import timing
		time.sleep(rank*0.2) #avoid mixing the final output, timing.stats() is independent of the sleep
		mprint( "#####  Worker "+str(rank)+"  ######")
		timing.stats() #specific numbers for -n4 and gabion.py


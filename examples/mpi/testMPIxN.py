#mpiexec -n 4 python testMPIxN.py
#mpiexec -n 4 python testMPIxN.py N M
# get non-MPI reference time using testMonolithic.py


'''
This script simulates spheres falling on a plate using a distributed memory approach based on mpi4py
The number of spheres assigned to one particular process (aka 'worker') is N*M, they form a regular array.
The master process (rank=0) has no spheres assigned presently; it is in charge of getting the total force on the plate (in more advanced simulations it could handle boundary conditions).
The number of subdomains depends on argument 'n' of mpiexec. Since rank=0 is not assigned a regular subdomain the total number of spheres is n*N*M


The logic is as follows:
1. Instanciate a complete, ordinary, yade scene with the plate and spheres (only done by rank=0)
2. Insert subdomains as special yade bodies. This is somehow similar to adding a clump body on the top of clump members
3. Broadcast this scene to all workers. In the initialization phase the workers will:
	- define the bounding box of their assigned bodies and return it to other workers
	- detect which assigned bodies are virtually in interaction with other domains (based on their bounding boxes) and communicate the lists to the relevant workers
	- erase the bodies which are neither assigned nor virtually interacting with the subdomain
4. Run a number of 'regular' iterations without re-running collision detection (verlet dist mechanism)
5. In each regular iteration the workers will:
	- calculate internal and cross-domains interactions
	- execute Newton on assigned bodies (modified Newton skips other domains)
	- send updated positions to other workers and partial force on floor to master

Yet to be implemented is the global update of domain bounds and new collision detection. It could be simply achieved by importing all bodies back in master process and re-running an initialization/distribution but there are certainly mmore efficient techniques to find.

Performance:
The default settings of testMPIxN.py and testMonolithic.py correspond to 90k spheres and 3 subdomains, for this problem size the MPI version runs approximately as fast as monolithic (actually a bit faster), the performance is expected to increase for more threads and/or more bodies per thread


'''

import sys
import time
from mpi4py import MPI
import pickle #just for measuring size of message before comm.irecv


comm = MPI.COMM_WORLD
rank = comm.Get_rank()
numThreads = comm.Get_size()


MAX_SUBDOMAIN = 8
NSTEPS=100 #turn it >0 to see time iterations, else only initilization
ACCUMULATE_FORCES=True #control force summation on master's body. FIXME: if false master goes out of sync since nothing is blocking rank=0 thread
VERBOSE_OUTPUT=False
SEND_SHAPES=True #if false only bodies' states are communicated between threads, else shapes as well (to be implemented)
N=300; M=100; #(columns, rows) per thread
ERASE_REMOTE = True #erase bodies not interacting wit a given subdomain? else keep dead clones of all bodies in each scene


#tags for mpi messages
_SCENE_=11
_SUBDOMAINSIZE_=12
_INTERSECTION_=13
_ID_STATE_SHAPE_=14
_FORCES_=15

#for coloring processes outputs differently
class bcolors:#https://stackoverflow.com/questions/287871/print-in-terminal-with-colors
    HEADER = '\033[95m'
    OKBLUE = '\033[94m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'

def mprint(m): #this one will print regardless of VERBOSE_OUTPUT
	if rank>0:
		print (bcolors.WARNING if rank==1 else bcolors.OKBLUE) +"worker"+str(rank)+": "+m+bcolors.ENDC
	else: print bcolors.HEADER+"master:" +m+bcolors.ENDC

def wprint(m):
	if not VERBOSE_OUTPUT: return
	mprint(m)


from os.path import expanduser
sys.path.append(expanduser('~')+'/yade/bin') # path where you have yadeimport.py
#print expanduser('~')+'/yade/bin'
# yadeimport.py is generated by `ln yade-versionNo yadeimport.py`, where yade-versionNo is the yade executable
from yadeimport import yade,sphere,box,Sphere,Body,Subdomain,Bo1_Subdomain_Aabb
#from yadeimport import *


#RULES:
	#- intersections[0] has 0-bodies (to which we need to send force)
	#- intersections[thisDomain] has ids of the other domains overlapping the current ones
	#- intersections[otherDomain] has ids of bodies in _current_ domain which are overlapping with other domain (for which we need to send updated pos/vel)


def receiveForces(subdomains):
	'''
	Accumulate forces from subdomains (only executed by master process), should happen after ForceResetter but before Newton and before any other force-dependent engine (e.g. StressController), could be inserted via yade's pyRunner
	'''
	if 1: #non-blocking:
		reqForces=[]
		for sd in subdomains:#would be better as a loop on subdomains directly, but we don't have those
			
			#wprint( "master getting forces from "+str(b.subdomain)+"(id="+str(b.id)+")")		
			reqForces.append(comm.irecv(None,sd, tag=_FORCES_))
			#wprint( "master got forces from "+str(b.subdomain)+": "+str(forces))		
		for r in reqForces:
			forces=r.wait()
			for ft in forces:		
				#wprint(  "adding force "+str(ft[1])+" to body "+str(ft[0]))
				O.forces.addF(ft[0],ft[1])
				O.forces.addT(ft[0],ft[2])
	else:
		for sd in subdomains:#would be better as a loop on subdomains directly, but we don't have those
			forces=comm.recv(source=sd, tag=_FORCES_)
			#wprint( "master got forces from "+str(b.subdomain)+": "+str(forces))		
			for ft in forces:
				#wprint(  "adding force "+str(ft[1])+" to body "+str(ft[0]))
				O.forces.addF(ft[0],ft[1])
				O.forces.addT(ft[0],ft[2])

def prepareScene():
	'''
	Turn bounding boxes on/off depending on rank
	'''
	for b in O.bodies:# unbound the bodies assigned to workers (not interacting directly with other bodies in master scene)
		if not b.isSubdomain and b.subdomain!=rank:
			b.bounded=False
			b.bound=None

def updateBounds(subdomains): #subdomains is the list of subdomains by body ids
	'''
	Update bounds of current subdomain, broadcast, and receive updated bounds from other subdomains
	Precondition: collider.boundDispatcher.__call__() 
	'''
	wprint( "updating bounds: "+str(subdomains))
	sharedBounds=[]
	if rank!=0:#this is not master process, update bounds and share
		subD=O.bodies[subdomains[rank-1]].shape #shorthand to shape of current subdomain
		subD.setMinMax()
		
		for worker in range(numThreads):
			if worker!=rank:
				wprint("sending "+str([subD.boundsMin,subD.boundsMax]))
				#comm.isend([subD.boundsMin,subD.boundsMax], dest=worker, tag=_SUBDOMAINSIZE_)
				req=comm.send([subD.boundsMin,subD.boundsMax], dest=worker, tag=_SUBDOMAINSIZE_)
				sharedBounds.append(req) #keep track of non-blocking messages sent 
	for sdId in subdomains:
		if O.bodies[sdId].subdomain!=rank:
			minmax=comm.recv(source=O.bodies[sdId].subdomain, tag=_SUBDOMAINSIZE_)
			wprint( "received mn,mx="+str(minmax[0])+" "+str(minmax[1])+" from "+str(O.bodies[sdId].subdomain))
			O.bodies[sdId].shape.boundsMin, O.bodies[sdId].shape.boundsMax = minmax[0],minmax[1]
	wprint( "receiving bounds")	
	#for req in sharedBounds: req.Wait()
	wprint( "bounds updated")
	
	
def getIntersections(subdomains):
	'''
	
	'''
	intersections=[[] for n in range(numThreads)]
	for sdId in subdomains:
		subdIdx=O.bodies[sdId].subdomain
		intrs=O.interactions.withBodyAll(sdId)
		#special case when we get interactions with current domain, only used to define interactions with master, otherwise some intersections would appear twice
		if subdIdx==rank:
			for i in intrs:
				otherId=i.id1 if i.id2==sdId else i.id2
				b=O.bodies[otherId]
				if b.subdomain==0: intersections[0].append(otherId)
			continue
		# normal case
		wprint( "found "+str(len(intrs))+" intersections")
		for i in intrs:
			otherId=i.id1 if i.id2==sdId else i.id2
			b=O.bodies[otherId]
			if b.subdomain!=rank: continue
			if b.isSubdomain: intersections[rank].append(subdIdx) #intersecting subdomain (will need to receive updated positions from there)
			else: intersections[subdIdx].append(otherId) #intersecting body (will need to send to otherSubdomain updated position of otherId
			
		#for master domain set list of interacting subdomains (could be handled above but for the sake of clarity complex if-else-if are avoided for now)
		if rank==0 and len(intersections[subdIdx])>0:
			intersections[0].append(subdIdx)
			
	return intersections


def integrate(intersections):
	'''
	This is integrating the subdomain assigned to current worker
	the master process is handled in the first section
	
	'''
	############## master process ############## 
	if rank==0:
		start=time.time()
		if ACCUMULATE_FORCES:
			#mprint ("master running until (after) interactionLoop")
			idx = O.engines.index(interactionLoop)
			for e in O.engines[0:idx+1]:
				if not isinstance(e,yade.Collider):#__call__() would trigger detection regardless of striding, we skip it here. TODO: trigger periodically based on the colliders of the subdomains 
					e.__call__()
			#wprint ("interactions by master:")
			#mprint("first part of the loop: "+str(time.time()-start)); start=time.time()
			#for i in O.interactions:#warning: this loop is significantly slow even when not printing - commented out
				#wprint(str(i.id1)+" "+str(i.id2)+" "+str(i.phys.normalForce[1]))
			#mprint("empty loop: "+str(time.time()-start)); start=time.time()
			receiveForces(intersections[0]) #update forces in the right place (just after interaction loop), could be also a pyRunner (see [#2])
			#mprint("sum forces: "+str(time.time()-start)); start=time.time()
			#mprint("getting forces: "+str(time.time()-start)); start=time.time()
			for e in O.engines[idx+1:]: e.__call__()
			#mprint("second part of the loop: "+str(time.time()-start)); start=time.time()
		else:
			O.step()
			#mprint("broadcast positions: "+str(time.time()-start)); start=time.time()
		
		for sd in intersections[0]:
			states= [[id,O.bodies[id].state,O.bodies[id].shape] for id in intersections[sd]]
			#wprint ( "bodies in 0 interacting with "+str(sd)+" (send):"+str(states))
			comm.send(states, dest=sd, tag=_ID_STATE_SHAPE_) 
		#mprint("broadcast positions: "+str(time.time()-start)); start=time.time()
		return
	
	############## workers ################
	#____0. run
	start=time.time()
	#wprint ( "worker"+str(rank)+" O.step()")
	O.step()
	#mprint("step done: "+str(time.time()-start)); start=time.time()
	if 0:
		wprint ( "interactions by worker "+str(rank)+": ")
		for i in O.interactions:
			wprint (str(i.id1)+" "+str(i.id2)+" "+str(i.phys.normalForce[1]))
	
	#____1. get ready to receive positions from other subdomains
	#wprint("listen states from (list of workers): "+str(intersections[rank]))
	pstates = []
	
	#print "________________ pickled size: ",len(pickle.dumps([0,O.bodies[0].state,O.bodies[0].shape], -1))*N*M
	#print "________________ minimal size: ",len(pickle.dumps([yade.Vector3(0,0,0) for k in range(4)], -1))*N*M
	buf = bytearray(1<<10)*M #heuristic guess, assuming number of intersecting is ~linear in the number of rows, needs 
	#buf = bytearray(1<<24) # 128 MB buffer, make it larger if needed. 
	for otherDomain in intersections[rank]:#this loop is superfluous with only one subdomain... will make sense with N. Precondition: subD.intersections is initialized
		#wprint( str(": getting states from ")+str(otherDomain))
		pstates.append( comm.irecv(buf,otherDomain, tag=_ID_STATE_SHAPE_))  #warning leaving buffer size undefined crash for large subdomains (MPI_ERR_TRUNCATE: message truncated)
	#mprint("prepared receive: "+str(time.time()-start)); start=time.time()
	
	#____2. broadcast new positions (should be non-blocking if n>2, else lock) - this includes subdomain bodies intersecting the current one, then broadcast forces to domain 0
	
	reqs=[]
	for k in range(0,len(intersections)):
		if k==rank: continue #don't broadcast to itself... OTOH this list intersections[rank] will be used to receive
		b_ids=intersections[k]
		states= [[id,O.bodies[id].state,O.bodies[id].shape] for id in b_ids] #would be empty in most cases if N>>1, for distant subdomains. Send empty list anyway? currently not
		
		if len(states)>0:#skip empty intersections, it means even the bounding boxes of the cooresponding subdomains do not overlap
			#wprint("sending states to "+str(k)+" b_ids="+str(b_ids))
			if k>0:
				comm.send(states, dest=k, tag=_ID_STATE_SHAPE_) #should be non-blocking if n>2, else lock
			#else #skip domain 0, it will get forces later
	#mprint("broadcast new positions: "+str(time.time()-start)); start=time.time()
			
	#send forces to domain 0
	if ACCUMULATE_FORCES:
		forces0=[[id,O.forces.f(id),O.forces.t(id)] for id in  intersections[0]]
		#wprint ("worker "+str(rank)+": sending "+str(len(forces0))+" "+str("forces to 0 "))
		comm.send(forces0, dest=0, tag=_FORCES_)
			
	#mprint("broadcast forces: "+str(time.time()-start)); start=time.time()
	#____2. get positions from other subdomains
	#wprint("get states from (list of workers): "+str(intersections[rank]))
	for ss in pstates:
		#wprint( str(": getting states from ")+str(kk))
		states=ss.wait()
		#wprint( str(": got "+str(len(states))+" states from ")+str(kk))
		for id,st,sh in states:
			if not O.bodies[id].bounded: print "ERROR: we are updating an unbounded body, should not happen"
			O.bodies[id].state=st
			O.bodies[id].shape=sh
	#mprint("updated remote states: "+str(time.time()-start)); start=time.time()

##########  CREATE A SCENE ############
if rank == 0: #this is the master process
	##### DEFINE A SCENE #########
	newton.gravity=(0,-10,0) #else nothing would move
	tsIdx=O.engines.index(timeStepper) #remove the automatic timestepper
	O.engines=O.engines[0:tsIdx]+O.engines[tsIdx+1:]
	O.dt=0.002 #very important, we don't want subdomains to use many different timesteps...

	if len(sys.argv)>1: #we then assume N,M are provided as 1st and 2nd cmd line arguments
		N=int(sys.argv[0]); M=int(sys.argv[1])
	
	for i in range(N*(numThreads-1)):#(numThreads-1) x N x M spheres, one thread is for master and will keep only the wall, others handle spheres	
		for j in range(M):
			yade.Omega().bodies.append(sphere((i+j/3000.,j,0),0.500)) #a small shift in x-positions of the rows to break symmetry
			
	WALL_ID=O.bodies.append(box(center=(0,-0.5,0),extents=(10000,0,10000),fixed=True))
	
	
	
##### INITIALIZE MPI #########
if rank == 0:
	#set subdomains
	for b in O.bodies:
		#print "____________ assigning ",int(b.state.pos[0]/N)+1 if isinstance(b.shape,Sphere) else 0,"to",b.id
		b.subdomain = int(b.state.pos[0]/N)+1 if isinstance(b.shape,Sphere) else 0
	
	#insert "meta"-bodies
	subdomains=[] #list subdomains by body ids
	for k in range(1,numThreads):
		domainBody=Body(shape=Subdomain(ids=[b.id for b in O.bodies if b.subdomain==k and isinstance(b.shape,Sphere)],subDomainIndex=k),subdomain=k) #note: not clear yet how shape.subDomainIndex and body.subdomain should interact, currently equal values
		domainBody.isSubdomain=True
		subdomains.append(O.bodies.append(domainBody))

	#tell the collider how to handle this new thing
	collider.boundDispatcher.functors=collider.boundDispatcher.functors+[Bo1_Subdomain_Aabb()]
	collider.verletDist = 0.3 #FIXME: in general yade set this one automatically but here it will only happen after collider.__call__() (i.e too late...)
	
	#BEGIN Garbage (should go to some init(), usually done in collider.__call__() but in the mpi case we want to collider.boundDispatcher.__call__() before collider.__call__()
	collider.boundDispatcher.sweepDist=collider.verletDist;
	collider.boundDispatcher.minSweepDistFactor=collider.minSweepDistFactor;
	collider.boundDispatcher.targetInterv=collider.targetInterv;
	collider.boundDispatcher.updatingDispFactor=collider.updatingDispFactor;
	#END Garbage
	
	#distribute work
	for worker in range(1,numThreads):
		comm.send(yade.Omega().sceneToString(), dest=worker, tag=_SCENE_) #sent with scene.subdomain=1, better make subdomain index a passed value so we could pass the sae string to every worker (less serialization+deserialization)
		
else:
	O.stringToScene(comm.recv(source=0, tag=_SCENE_)) #receive a scene pre-processed by master (i.e. with appropriate body.subdomain's)  
	#print "worker 1 received",len(O.bodies),"bodies (verletDist=",collider.verletDist,")"
	thisSubdomain=rank
	O._sceneObj.subdomain = thisSubdomain
	
	domainBody=None
	assignedBodies=[]
	subdomains=[] #list of subdomains by body id
	for b in O.bodies:
		if b.isSubdomain:
			subdomains.append(b.id)
			if b.shape.subDomainIndex==rank: domainBody=b
		elif b.subdomain==rank: assignedBodies.append(b.id)
	if domainBody==None: print "SUBDOMAIN NOT FOUND FOR RANK=",rank

prepareScene()
collider.boundDispatcher.__call__()
updateBounds(subdomains) 
	
if rank==0:#master domain
	#update bounds wrt. updated subdomain(s) min/max and unbounded bodies	
	collider.__call__() #see [1]
	
	intersections=getIntersections(subdomains)

	for worker in range(1,numThreads):#FIXME: we actually don't need so much data since at this stage the states are unchanged and the list is used to re-bound intersecting bodies, this is only done in the initialization phase, though
		states= [[id,O.bodies[id].state,O.bodies[id].shape] for id in intersections[worker]]
		wprint("sending states to "+str(worker)+" ("+str(len(states))+" bodies)")
		comm.send(states, dest=worker, tag=_ID_STATE_SHAPE_) #sent with scene.subdomain=1
	
	if ERASE_REMOTE:#only bottom wall and subdomins will be left
		numBodies = len(O.bodies)
		for id in range(numBodies):
			if not O.bodies[id].bounded:
				O.bodies.erase(id)
	
	##### RUN MPI #########
	integrate(intersections) #a first iteration not included in timing, to be sure we measure regular iterations only
	
	start=time.time()	
	for iter in range(NSTEPS):
		#wprint ("\n\n ______________START MASTER STEP "+str(O.iter) +" ")
		integrate(intersections)
		#wprint ( "MASTER TERMINATED WITH TOTAL FORCE ON FLOOR="+str(O.forces.f(WALL_ID)[1]))
	totTime = (time.time()-start)
	
	mass=0
	for b in O.bodies: mass+=b.state.mass
	mprint ( "TOTAL FORCE ON FLOOR="+str(O.forces.f(WALL_ID)[1])+" vs. total weight "+str(mass*10) )
	#bodies counted without floor and without subdomains
	mprint ( "done "+str(N*M*(numThreads-1))+" bodies in "+str(totTime)+ " sec, at "+str(N*M*NSTEPS*(numThreads-1)/totTime)+" body*iter/second")
	mprint ( "num. collision detection "+str(collider.numAction))
    
else:#workers
	#wprint("getting 0-states")
	states=comm.recv(source=0, tag=_ID_STATE_SHAPE_)
	#wprint("received"+str(states))
	for id,st,sh in states:
		#print "worker updating body ",id
		b=O.bodies[id]
		b.state=st
		b.shape=sh
		b.bounded=True #bound again since this body is part of interacting neighbourhood
	
	collider.__call__() #see [1]
	#wprint("getting intersections")
	subD=domainBody.shape
	subD.intersections=getIntersections(subdomains)
	#wprint("intersections acquired")
	
	reqs=[]
	for worker in subD.intersections[rank]:
		states= [[id,O.bodies[id].state,O.bodies[id].shape] for id in subD.intersections[worker]]
		buf = bytearray(1<<11)*M
		reqs.append(comm.irecv(buf, worker, tag=_ID_STATE_SHAPE_))
		
	for worker in subD.intersections[rank]:
		states= [[id,O.bodies[id].state,O.bodies[id].shape] for id in subD.intersections[worker]]
		#wprint("sending "+str(len(states))+" states to "+str(worker))
		comm.send(states, dest=worker, tag=_ID_STATE_SHAPE_) 
		#wprint("sent")
	
	for pstates in reqs:
		states=pstates.wait()
		#wprint("received "+str(len(states))+ " states")
		for id,st,sh in states:
			#print "worker : updating body ",id
			b=O.bodies[id]
			b.state=st
			b.shape=sh
			b.bounded=True #bound again since this body is part of interacting neighbourhood
			
	if ERASE_REMOTE:
		numBodies = len(O.bodies)
		for id in range(numBodies):
			if not O.bodies[id].bounded:
				O.bodies.erase(id)
	
	##### RUN MPI #########
	integrate(subD.intersections)
	for iter in range(NSTEPS):
		#wprint ( "\n\n _______START WORKER STEP "+str(O.iter) +"__________ ")
		integrate(subD.intersections)
	#mprint ( "num. collision detection "+str(collider.numAction))


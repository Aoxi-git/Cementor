#mpiexec -n 2 python testMPI.py 200 20
# get non-MPI reference time using testMonolithic.py
import sys
import time
from os.path import expanduser
sys.path.append(expanduser('~')+'/yade/bin') # path where you have yadeimport.py
# yadeimport.py is generated by `ln yade-versionNo yadeimport.py`, where yade-versionNo is the yade executable
#print expanduser('~')+'/yade/bin'
from mpi4py import MPI

comm = MPI.COMM_WORLD
rank = comm.Get_rank()


MAX_SUBDOMAIN = 8
NSTEPS=400 #turn it >0 to see time iterations, else only initilization
VERBOSE_OUTPUT=False


#tags for mpi messages
_SCENE_=11
_SUBDOMAINSIZE_=12
_INTERSECTION_=13
#_STATE_SHAPE_
_ID_STATE_SHAPE_=14
_FORCES_=15

#for coloring processes outputs differently
class bcolors:#https://stackoverflow.com/questions/287871/print-in-terminal-with-colors
    HEADER = '\033[95m'
    OKBLUE = '\033[94m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'

#print bcolors.WARNING + "Warning: No active frommets remain. Continue?"+ bcolors.ENDC

def wprint(m):
	if not VERBOSE_OUTPUT: return
	if rank>0:
		print bcolors.WARNING +"worker"+str(rank)+": "+m+bcolors.ENDC
	else: print bcolors.HEADER+"master:" +m+bcolors.ENDC
	
def mprint(m): #this one will print regardless of VERBOSE_OUTPUT
	if rank>0:
		print bcolors.WARNING +"worker"+str(rank)+": "+m+bcolors.ENDC
	else: print bcolors.HEADER+"master:" +m+bcolors.ENDC



#based on "O" some what follows looks as usual to yade users
from yadeimport import *
#O=yade.Omega()

#RULES:
	#- intersections[0] has 0-bodies (to which we need to send force)
	#- intersections[thisDomain] has ids of the other domains overlapping the current ones
	#- intersections[otherDomain] has ids of bodies in _current_ domain which are overlapping with other domain (for which we need to send updated pos/vel)

#def initMPI(subD):
	##set intersections and list of 
	
	#thisSubdomain=O._sceneObj.subdomain
	#print "worker 1 got subdomain ",thisSubdomain
	
	#sub1=[b.id for b in O.bodies if b.subdomain==thisSubdomain]
	#print "worker 1= sub1=",sub1
	##retrieve the subdomain and unbound external bodies (intersecting ones revided later)
	#domainBody1=None
	#for b in O.bodies:
		#if b.subdomain != thisSubdomain:
			#b.bounded=False
			#b.bound=None
		#if isinstance(b.shape,yade.Subdomain) and b.shape.subDomainIndex==thisSubdomain: domainBody1=b
			
		##print b.shape, yade.Subdomain, b.subdomain, thisSubdomain, domainBody1
	#if domainBody1==None: print "SUBDOMAIN NOT FOUND FOR RANK=",rank
	#subD = domainBody1.shape
	#subD.intersections=[[]]*MAX_SUBDOMAIN
	
	
def receiveForces():
	'''
	Accumulate forces from subdomains (only executed by master process), should happen after ForceResetter but before Newton and before any other force-dependent engine (e.g. StressController), could be inserted via yade's pyRunner
	'''
	for b in O.bodies:#would be better as a loop on subdomains directly, but we don't have those
		if isinstance(b.shape,Subdomain): 
			wprint( "master getting forces from "+str(b.subdomain)+"(id="+str(b.id)+")")
			forces = comm.recv(source=b.subdomain, tag=_FORCES_)
			wprint( "master got forces from "+str(b.subdomain)+": "+str(forces))
			for ft in forces:
				wprint(  "adding force "+str(ft[1])+" to body "+str(ft[0]))
				O.forces.addF(ft[0],ft[1])
				O.forces.addT(ft[0],ft[2])

def integrate(subD):
	'''
	This is integrating the subdomain assigned to current worker
	if subD==None this is the master process, handled in the first section
	NOTE: subD could be retrieved by each corresponding worker if not passed as a variable but it would mean another loop on bodies to find it, hence better pass it
	
	'''
	
	############## master process ############## 
	if subD==None:
		start=time.time()
		wprint ("master running until (after) interactionLoop")
		idx = O.engines.index(interactionLoop)
		for e in O.engines[0:idx+1]: e.__call__()
		wprint ("interactions by master:")
		#mprint("first part of the loop: "+str(time.time()-start)); start=time.time()
		for i in O.interactions:#warning: this loop is significantly slow even when not printing - commented out
			wprint(str(i.id1)+" "+str(i.id2)+" "+str(i.phys.normalForce[1]))
		#mprint("empty loop: "+str(time.time()-start)); start=time.time()
		receiveForces() #update forces in the right place (just after interaction loop), could be also a pyRunner (see [#2])
		#mprint("getting forces: "+str(time.time()-start)); start=time.time()
		for e in O.engines[idx+1:]: e.__call__()
		#mprint("second part of the loop: "+str(time.time()-start)); start=time.time()
		for b in O.bodies:
			if b.isSubdomain:
				N = b.subdomain
				intersect_0_N=[i.id1 if i.id2==b.id else i.id2 for i in O.interactions.withBodyAll(b.id) ]
				#exclude other subdomains from message since they can go through worker-to-worker communications
				states= [[id,O.bodies[id].state,O.bodies[id].shape] for id in intersect_0_N if not O.bodies[id].isSubdomain]
				wprint ( "master: bodies in 0 interacting with "+str(N)+" (send):"+str(intersect_0_N))
				comm.send(states, dest=N, tag=_ID_STATE_SHAPE_) 
		#mprint("broadcast positions: "+str(time.time()-start)); start=time.time()
		return
	
	############## workers ################
	#____0. run
	wprint ( "worker"+str(O._sceneObj.subdomain)+" O.step()")
	O.step()
	wprint ( "interactions by worker "+str(O._sceneObj.subdomain)+": ")
	for i in O.interactions:
		wprint (str(i.id1)+" "+str(i.id2)+" "+str(i.phys.normalForce[1]))
	
	#____1. broadcast new positions (should be non-blocking if n>2, else lock) - this includes subdomain bodies intersecting the current one, then broadcast forces to domain 0
	
	#this loop is superfluous with only one subdomain... will make sense with N. Precondition: subD.intersections is initialized
	#NOTE: we skip special domain 0 (?)n it will get forces later
	for k in range(1,len(subD.intersections)):
		if k==O._sceneObj.subdomain: continue #don't broadcast to itself... OTOH this list subD.intersections[subD.subdomain] will be used to receive
		b_ids=subD.intersections[k]
		states= [[id,O.bodies[id].state,O.bodies[id].shape] for id in b_ids] #would be empty in most cases if N>>1, for distant subdomains. Send empty list anyway? currently not
		if len(states)>0:#skip empty intersections, it means even the bounding boxes of the cooresponding subdomains do not overlap
			if k>0:#hypothetical in this script...
				comm.send(states, dest=k, tag=_ID_STATE_SHAPE_) #should be non-blocking if n>2, else lock
			#else #skip domain 0, it will get forces later
			
	#send forces to domain 0
	forces0=[[id,O.forces.f(id),O.forces.t(id)] for id in  subD.intersections[0]]
	wprint ("worker "+str(O._sceneObj.subdomain)+": sending "+str(len(forces0))+" "+str("forces to 0 "))
	comm.send(forces0, dest=0, tag=_FORCES_)
			
			
	#____2. get positions from other subdomains
	for otherDomain in subD.intersections[O._sceneObj.subdomain]:#this loop is superfluous with only one subdomain... will make sense with N. Precondition: subD.intersections is initialized
		wprint( "worker "+str(O._sceneObj.subdomain)+str(": getting positions from ")+str(otherDomain))
		states = comm.recv(source=otherDomain, tag=_ID_STATE_SHAPE_)
		for id,st,sh in states:
			wprint( ("worker ")+str(O._sceneObj.subdomain)+str(": updating body ")+str(id))
			if not O.bodies[id].bounded: print "ERROR: we are updating an unbounded body, should not happen"
			O.bodies[id].state=st
			O.bodies[id].shape=sh
			
	#____3. get positions from master
	wprint( "worker "+str(O._sceneObj.subdomain)+": getting positions from master")
	states = comm.recv(source=0, tag=_ID_STATE_SHAPE_)
	for id,st,sh in states:
		wprint( "worker "+str(O._sceneObj.subdomain)+": updating body "+str(id)+" with new pos="+str(st.pos))
		if not O.bodies[id].bounded: wprint( "ERROR: we are updating an unbounded body, should not happen")
		O.bodies[id].state=st
		O.bodies[id].shape=sh


if rank == 0: #this is the master process
	##### DEFINE A SCENE #########
	newton.gravity=(0,-10,0) #else nothing would move
	O.dynDt=0
	timeStepper.dead=True
	O.dt=0.002 #very important, we don't want subdomains to use many different timesteps...

	if len(sys.argv)>1: #we then assume N,M are provided as 1st and 2nd cmd line arguments
		N=int(sys.argv[0]); M=int(sys.argv[1])
	else:
		N=4; M=2; #(columns, rows)
	sub1=[] #ids in subdomain 1
	for i in range(N): #NxM spheres
		for j in range(M):
			id=yade.Omega().bodies.append(sphere((i+j/3.,j,0),0.51))
			if i<((N+1)/2.): sub1.append(id)
	WALL_ID=O.bodies.append(box(center=(0,-0.5,0),extents=(10000,0,10000),fixed=True))
	
	##### INITIALIZE MPI #########
	
	#set subdomain
	for k in sub1: O.bodies[k].subdomain=1 
	#insert a "meta"-body 
	domainBody1=Body(shape=Subdomain(ids=[b.id for b in O.bodies if b.subdomain==1 and isinstance(b.shape,Sphere)],subDomainIndex=1),subdomain=1) #note: not clear yet how shape.subDomainIndex and body.subdomain should interact
	domainBody1.isSubdomain=True
	subIdx1=O.bodies.append(domainBody1)
	#tell the collider how to handle this new thing
	collider.boundDispatcher.functors=collider.boundDispatcher.functors+[Bo1_Subdomain_Aabb()]
	collider.verletDist = 0.3 #FIXME: in general yade set this one automatically but here it will only happen after collider.__call__() (i.e too late...)
	
	#BEGIN Garbage (should go to some init(), usually done in collider.__call__() but in the mpi case we want to collider.boundDispatcher.__call__() before collider.__call__()
	collider.boundDispatcher.sweepDist=collider.verletDist;
	collider.boundDispatcher.minSweepDistFactor=collider.minSweepDistFactor;
	collider.boundDispatcher.targetInterv=collider.targetInterv;
	collider.boundDispatcher.updatingDispFactor=collider.updatingDispFactor;
	#END Garbage
	
	#distribute work
	#print "master: sending scene"
	O._sceneObj.subdomain=1
	comm.send(yade.Omega().sceneToString(), dest=1, tag=_SCENE_) #sent with scene.subdomain=1, better make subdomain index a passed value so we could pass the sae string to every worker (less serialization+deserialization)
	O._sceneObj.subdomain=0
	for id in sub1:#make 1-bodies unbounded (not interacting directly with other bodies in this scene)
		O.bodies[id].bounded=False
		O.bodies[id].bound=None
	#print "master: get mn1,mx1="
	
	
	minmax=comm.recv(source=1, tag=_SUBDOMAINSIZE_)
	#print "master: received mn1,mx1=",minmax[0],minmax[1]
	domainBody1.shape.boundsMin, domainBody1.shape.boundsMax = minmax[0],minmax[1]
	
	#update bounds wrt. updated subdomain(s) min/max and unbounded bodies
	collider.boundDispatcher.__call__()
	collider.__call__() #see [1]
	intersect_0_1=[i.id1 if i.id2==domainBody1.id else i.id2 for i in O.interactions.withBodyAll(domainBody1.id) ]
	#print "master: bodies in 0 interacting with 1 (send):",intersect_0_1
	comm.send(intersect_0_1, dest=1, tag=_INTERSECTION_) #sent with scene.subdomain=1
	#print "master sending states to 1"
	states= [[id,O.bodies[id].state,O.bodies[id].shape] for id in intersect_0_1]
	comm.send(states, dest=1, tag=_ID_STATE_SHAPE_) #sent with scene.subdomain=1
	
	##### RUN MPI #########
	start=time.time()
	
	for iter in range(NSTEPS):
		wprint ("\n\n START MASTER STEPS \n\n")
		integrate(None)
		wprint ( "MASTER TERMINATED WITH TOTAL FORCE ON FLOOR="+str(O.forces.f(WALL_ID)[1]))
	totTime = (time.time()-start)
	
	mass=0
	for b in O.bodies: mass+=b.state.mass
	mprint ( "TOTAL FORCE ON FLOOR="+str(O.forces.f(WALL_ID)[1])+" vs. total weight "+str(mass*10) )
	mprint ( "master: done "+str(N*M)+" bodies in "+str(totTime)+ " sec, at "+str(N*M*NSTEPS/totTime)+" body*iter/second")
    
elif rank == 1:
	
	##### INITIALIZE MPI #########
	O.stringToScene(comm.recv(source=0, tag=_SCENE_)) #receive a scene pre-processed by master (i.e. with appropriate body.subdomain's)  
	#print "worker 1 received",len(O.bodies),"bodies (verletDist=",collider.verletDist,")"
	thisSubdomain=O._sceneObj.subdomain
	#print "worker 1 got subdomain ",thisSubdomain
	
	sub1=[b.id for b in O.bodies if b.subdomain==thisSubdomain]
	#print "worker 1= sub1=",sub1
	#retrieve the subdomain and unbound external bodies (intersecting ones revided later)
	domainBody1=None
	for b in O.bodies:
		if b.subdomain != thisSubdomain:
			b.bounded=False
			b.bound=None
		if isinstance(b.shape,yade.Subdomain) and b.shape.subDomainIndex==thisSubdomain: domainBody1=b
			
		#print b.shape, yade.Subdomain, b.subdomain, thisSubdomain, domainBody1
	if domainBody1==None: print "SUBDOMAIN NOT FOUND FOR RANK=",rank
	subD = domainBody1.shape
	subD.intersections=[[]]*MAX_SUBDOMAIN
			
	#subIdx1=O.bodies.append(domainBody1)
	##O.bodies[subIdx1].shape.subDomainIndex=subIdx1 #points back to position in O.bodies?
	#print "worker 1 collider.boundDispatcher (verletDist=",collider.verletDist,")"
	collider.boundDispatcher.__call__()
	#print "worker 1 dispatched bounds (verletDist=",collider.verletDist,")"
	#set bounding size, and return to other threads (now only master one)
	domainBody1.shape.setMinMax() 
	#print "worker 1 sending mn1,mx1=",domainBody1.shape.boundsMin,domainBody1.shape.boundsMax
	comm.send([domainBody1.shape.boundsMin,domainBody1.shape.boundsMax], dest=0, tag=_SUBDOMAINSIZE_)
	#print "worker 1 getting ids of image 0-bodies"
	intersect_0_1=comm.recv(source=0, tag=_INTERSECTION_)
	#print "worker 1 got",intersect_0_1,", they will be re-bounded"
	k=0
	subD.intersections=subD.intersections[:k]+[intersect_0_1]+subD.intersections[k+1:] #note: setting k-th element, here 0th...
	
	
	# turn intersecting bodies ON
	for id in subD.intersections[0]:
		O.bodies[id].bounded=True
	#print "worker 1: interactions before colliding:"
	#for i in O.interactions.all():
		#print i.id1,i.id2
	collider.__call__()
	#print "worker 1: collider found interactions:"
	#for i in O.interactions.all():
		#loc= "internal" if O.bodies[i.id1].subdomain==O.bodies[i.id2].subdomain else "interfacial"
		#print i.id1,i.id2,"(",loc,")"
	#print "worker 1: receiving + updating pos/vel of image particles"
	for id,st,sh in comm.recv(source=0, tag=_ID_STATE_SHAPE_):
		#print "worker 1: updating body ",id
		b=O.bodies[id]
		b.state=st
		b.shape=sh
		
	#print "worker 1: running one step"
	#O.run(1,1)
	#forces0=[[id,O.forces.f(b.id),O.forces.t(b.id)] for id in  subD.intersections[0]]
	#print "worker 1: sending 0-forces",forces0
	#comm.send(forces0, dest=0, tag=_FORCES_)
	
	##### RUN MPI #########
	for iter in range(NSTEPS):
		wprint ( "\n\n START WORKER STEPS \n\n")
		integrate(subD)

	

	
	
	
	
    
    
    




#ISSUE1:
    #comm.send(yade.Scene(), dest=1, tag=12) 
  #File "MPI/Comm.pyx", line 1130, in mpi4py.MPI.Comm.send (src/mpi4py.MPI.c:94609)
  #File "MPI/msgpickle.pxi", line 187, in mpi4py.MPI.PyMPI_send (src/mpi4py.MPI.c:35845)
  #File "MPI/msgpickle.pxi", line 88, in mpi4py.MPI._p_Pickle.dump (src/mpi4py.MPI.c:34491)
#TypeError: No to_python (by-value) converter found for C++ type: std::__cxx11::list<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >


##ISSUE2 / OPTIMIZATION:
#We could keep all bodies present in all scenes as soon as Newton will skip other subdomains.
#To skip interactions outside the current subdomain it would be enough to make bodies from other domains unbounded.
#The problem is the collider still sort bounds for unbounded bodies (using positions), which will generate a massive excess of bounds



#OPTIMIZATIONS
'''
- [1] no real need to run collider action ('collider.__call__()') twice to update subdomains bounds. We could dispatch+setMinMax+sort.
Alternatively, setMinMax could be optimized by taking first and last bounds after sort. In such case sort+setMinMax+sort would be needed. Note that the second __call__() should be faster since it uses pre-sorted bounds.
'''
